{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CQ312X78t9E2",
    "outputId": "5571164f-37d1-4b53-a702-5c1c98c1067e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6tW88uDUuy-L"
   },
   "outputs": [],
   "source": [
    "# !pip install vit_keras\n",
    "\n",
    "# !pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r7roS7a81_q1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, Input, Lambda, BatchNormalization, GlobalAveragePooling2D, AveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\"\"\"\n",
    "## Hyperparameters\n",
    "\"\"\"\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "margin = 1  # Margin for constrastive loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RkveFyN0rqWu"
   },
   "outputs": [],
   "source": [
    "save_weights_path = \"/content/drive/MyDrive/Graduation_Project/model_weight_Tiamese_mobile_2.hdf5\"\n",
    "load_weights_path = \"/content/drive/MyDrive/Graduation_Project/model_weight_Tiamese_mobile.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EA4TlBDi2Kqo"
   },
   "outputs": [],
   "source": [
    "def load_data(path,input_shape):\n",
    "    image_list=[]\n",
    "    label_list=[]\n",
    "    classes=['No_Finding', 'Enlarged_Cardiomediastinum', 'Cardiomegaly',\n",
    "       'Lung_Opacity', 'Lung_Lesion', 'Edema', 'Consolidation', 'Pneumonia',\n",
    "       'Atelectasis', 'Pneumothorax', 'Pleural_Effusion', 'Pleural_Other',\n",
    "       'Fracture', 'Support_Devices']\n",
    "    for category in classes:\n",
    "        picList= os.listdir(path+\"/\"+str(category))\n",
    "        for pic in tqdm(picList):\n",
    "            # image= cv2.imread(path+\"/\"+str(category)+\"/\"+pic)\n",
    "            # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            # image= cv2.resize(image,input_shape)/255.0\n",
    "            image_list.append(path+\"/\"+str(category)+\"/\"+pic)\n",
    "            label_list.append(classes.index(category))\n",
    "\n",
    "    image_df = pd.DataFrame({'Path':image_list, 'Label':label_list})\n",
    "\n",
    "   # return np.array(image_list),np.array(label_list)\n",
    "    return image_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WceR2ZAE4Dvf",
    "outputId": "b0ed6918-e1cc-41fe-e6a0-3619b0cd1244"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 480/480 [00:00<00:00, 131405.65it/s]\n",
      "100%|██████████| 480/480 [00:00<00:00, 555997.22it/s]\n",
      "100%|██████████| 480/480 [00:00<00:00, 469292.76it/s]\n",
      "100%|██████████| 480/480 [00:00<00:00, 727598.81it/s]\n",
      "100%|██████████| 480/480 [00:00<00:00, 232855.18it/s]\n",
      "100%|██████████| 480/480 [00:00<00:00, 772253.90it/s]\n",
      "100%|██████████| 480/480 [00:00<00:00, 579357.10it/s]\n",
      "100%|██████████| 296/296 [00:00<00:00, 415222.07it/s]\n",
      "100%|██████████| 480/480 [00:00<00:00, 268471.25it/s]\n",
      "100%|██████████| 480/480 [00:00<00:00, 275186.70it/s]\n",
      "100%|██████████| 480/480 [00:00<00:00, 558155.23it/s]\n",
      "100%|██████████| 214/214 [00:00<00:00, 577593.99it/s]\n",
      "100%|██████████| 480/480 [00:00<00:00, 593533.58it/s]\n",
      "100%|██████████| 480/480 [00:00<00:00, 539749.58it/s]\n"
     ]
    }
   ],
   "source": [
    "train = load_data('/content/drive/MyDrive/Graduation_Project/Train', (224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BYFaAeakeEno",
    "outputId": "03106590-0501-4796-f51e-f75493228e36"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:00<00:00, 333322.17it/s]\n",
      "100%|██████████| 120/120 [00:00<00:00, 412216.61it/s]\n",
      "100%|██████████| 120/120 [00:00<00:00, 285003.67it/s]\n",
      "100%|██████████| 120/120 [00:00<00:00, 387166.52it/s]\n",
      "100%|██████████| 120/120 [00:00<00:00, 447392.43it/s]\n",
      "100%|██████████| 120/120 [00:00<00:00, 342392.16it/s]\n",
      "100%|██████████| 120/120 [00:00<00:00, 331129.26it/s]\n",
      "100%|██████████| 75/75 [00:00<00:00, 253687.74it/s]\n",
      "100%|██████████| 120/120 [00:00<00:00, 278845.70it/s]\n",
      "100%|██████████| 120/120 [00:00<00:00, 325350.02it/s]\n",
      "100%|██████████| 120/120 [00:00<00:00, 277156.65it/s]\n",
      "100%|██████████| 54/54 [00:00<00:00, 222269.30it/s]\n",
      "100%|██████████| 120/120 [00:00<00:00, 279620.27it/s]\n",
      "100%|██████████| 120/120 [00:00<00:00, 297996.73it/s]\n"
     ]
    }
   ],
   "source": [
    "val = load_data('/content/drive/MyDrive/Graduation_Project/Val', (224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g5hU9dxOPGnP"
   },
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, train_df, input_shape=224, batch_size=32, shuffle=True):\n",
    "        self.train_df = train_df\n",
    "        self.input_shape = input_shape\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(self.train_df.shape[0] / self.batch_size)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        'Generate one batch of data'\n",
    "        pairs, labels = self.__data_generation()\n",
    "        pairs_1=pairs[:, 0]\n",
    "        pairs_2=pairs[:, 1]\n",
    "\n",
    "        return [pairs_1,pairs_2], labels\n",
    "\n",
    "    \n",
    "    def make_pairs(self, x, y):\n",
    "        \"\"\"Creates a tuple containing image pairs with corresponding label.\n",
    "        Arguments:\n",
    "            x: List containing images, each index in this list corresponds to one image.\n",
    "            y: List containing labels, each label with datatype of `int`.\n",
    "        Returns:\n",
    "            Tuple containing two numpy arrays as (pairs_of_samples, labels),\n",
    "            where pairs_of_samples' shape is (2len(x), 2,n_features_dims) and\n",
    "            labels are a binary array of shape (2len(x)).\n",
    "        \"\"\"\n",
    "\n",
    "        classes = np.unique(y)\n",
    "        #digit_indices = [np.where(y == i)[0] for i in classes]\n",
    "\n",
    "        pairs = []\n",
    "        labels = []\n",
    "\n",
    "        for idx1 in range(len(x)):\n",
    "            # add a matching example\n",
    "            x1 = x[idx1]\n",
    "            label1 = y[idx1]\n",
    "            idx2 = random.choice(np.where(y== label1)[0])\n",
    "            x2 = x[idx2]\n",
    "\n",
    "            pairs += [[x1, x2]]\n",
    "            labels += [1]\n",
    "\n",
    "            # add a non-matching example\n",
    "            label2 = random.choice(classes)\n",
    "            while label2 == label1:\n",
    "                label2 = random.choice(classes)\n",
    "\n",
    "            idx2 = random.choice(np.where(y== label2)[0])\n",
    "            x2 = x[idx2]\n",
    "\n",
    "            pairs += [[x1, x2]]\n",
    "            labels += [0]\n",
    "\n",
    "        return np.array(pairs), np.array(labels).astype(\"float32\")\n",
    "\n",
    "    def get_images(self, sample_df):\n",
    "        X = []\n",
    "        for i in range(sample_df.shape[0]):\n",
    "            image= cv2.imread(sample_df.iloc[i])\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image= cv2.resize(image, (self.input_shape, self.input_shape))/255.0\n",
    "            X.append(image)\n",
    "        \n",
    "        return np.array(X)\n",
    "\n",
    "    def __data_generation(self):\n",
    "        'Generates data containing batch_size samples' \n",
    "        # Initialization\n",
    "        indices = np.arange(self.train_df.shape[0])\n",
    "        idx = np.random.choice(indices, size=self.batch_size)\n",
    "        X = self.get_images(self.train_df.iloc[idx, 0])\n",
    "        y = self.train_df.iloc[idx, 1].values\n",
    "        pairs, labels=self.make_pairs(X,y)\n",
    "\n",
    "        return pairs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CcwsLzrL2Lgo"
   },
   "outputs": [],
   "source": [
    "train_gen = DataGenerator(train, input_shape=224, batch_size=32, shuffle=True)\n",
    "\n",
    "# make validation pairs\n",
    "val_gen = DataGenerator(val, input_shape=224, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QBcqqGS82LuX"
   },
   "outputs": [],
   "source": [
    "def euclidean_distance(vects):\n",
    "    \"\"\"Find the Euclidean distance between two vectors.\n",
    "    Arguments:\n",
    "        vects: List containing two tensors of same length.\n",
    "    Returns:\n",
    "        Tensor containing euclidean distance\n",
    "        (as floating point value) between vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    x, y = vects\n",
    "    sum_square = tf.math.reduce_sum(tf.math.square(x - y), axis=1, keepdims=True)\n",
    "    return tf.math.sqrt(tf.math.maximum(sum_square, tf.keras.backend.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lIMXF36O2L1f"
   },
   "outputs": [],
   "source": [
    "embedding_network=Sequential()\n",
    "embedding_network.add(Conv2D(64, (3, 3), activation=\"relu\"))\n",
    "embedding_network.add(BatchNormalization())\n",
    "embedding_network.add(AveragePooling2D(pool_size=(2, 2)))\n",
    "embedding_network.add(Conv2D(32, (3, 3), activation=\"relu\"))\n",
    "embedding_network.add(BatchNormalization())\n",
    "embedding_network.add(AveragePooling2D(pool_size=(2, 2)))\n",
    "embedding_network.add(Flatten())\n",
    "embedding_network.add(Dense(1024, activation=\"relu\"))\n",
    "\n",
    "\n",
    "\n",
    "input_1 = Input((224, 224, 3))\n",
    "input_2 = Input((224, 224, 3))\n",
    "\n",
    "# As mentioned above, Siamese Network share weights between\n",
    "# tower networks (sister networks). To allow this, we will use\n",
    "# same embedding network for both tower networks.\n",
    "tower_1 = embedding_network(input_1)\n",
    "tower_2 = embedding_network(input_2)\n",
    "\n",
    "merge_layer = Lambda(euclidean_distance)([tower_1, tower_2])\n",
    "normal_layer = BatchNormalization()(merge_layer)\n",
    "output_layer = Dense(1, activation=\"sigmoid\")(normal_layer)\n",
    "siamese = keras.Model(inputs=[input_1, input_2], outputs=output_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iPN3bQkf2L9n",
    "outputId": "c16611d9-e274-4da9-bfae-c1c29247d019"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 1024)         95573152    ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 1)            0           ['sequential[0][0]',             \n",
      "                                                                  'sequential[1][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 1)           4           ['lambda[0][0]']                 \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            2           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 95,573,158\n",
      "Trainable params: 95,572,964\n",
      "Non-trainable params: 194\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "siamese.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CYD8Frxq2MEf"
   },
   "outputs": [],
   "source": [
    "def loss(margin=1):\n",
    "    \"\"\"Provides 'constrastive_loss' an enclosing scope with variable 'margin'.\n",
    "  Arguments:\n",
    "      margin: Integer, defines the baseline for distance for which pairs\n",
    "              should be classified as dissimilar. - (default is 1).\n",
    "  Returns:\n",
    "      'constrastive_loss' function with data ('margin') attached.\n",
    "  \"\"\"\n",
    "\n",
    "    # Contrastive loss = mean( (1-true_value) * square(prediction) +\n",
    "    #                         true_value * square( max(margin-prediction, 0) ))\n",
    "    def contrastive_loss(y_true, y_pred):\n",
    "        \"\"\"Calculates the constrastive loss.\n",
    "      Arguments:\n",
    "          y_true: List of labels, each label is of type float32.\n",
    "          y_pred: List of predictions of same length as of y_true,\n",
    "                  each label is of type float32.\n",
    "      Returns:\n",
    "          A tensor containing constrastive loss as floating point value.\n",
    "      \"\"\"\n",
    "\n",
    "        square_pred = tf.math.square(y_pred)\n",
    "        margin_square = tf.math.square(tf.math.maximum(margin - (y_pred), 0))\n",
    "        return tf.math.reduce_mean(\n",
    "            (1 - y_true) * square_pred + (y_true) * margin_square\n",
    "        )\n",
    "\n",
    "    return contrastive_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pYYhnkCs9URv"
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(save_weights_path, monitor='val_auc', verbose=1, save_best_only=True, mode='max', save_freq='epoch')\n",
    "early = EarlyStopping(monitor=\"val_auc\", mode='max', patience=10, restore_best_weights=True)\n",
    "reduceLROnPlato = ReduceLROnPlateau(monitor=\"val_auc\", factor=0.1, patience=5, verbose=1, mode='max')\n",
    "callbacks_list = [checkpoint, early, reduceLROnPlato]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shNxt0wN2ML4"
   },
   "outputs": [],
   "source": [
    "siamese.compile(loss=loss(margin=margin), optimizer=Adam(learning_rate=0.1), metrics=[\"accuracy\", tf.keras.metrics.AUC(multi_label = False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NBe_yGrm2MUH",
    "outputId": "c0727be3-acc2-4eb8-fc7a-6131ee0a3b05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "195/195 [==============================] - ETA: 0s - loss: 0.2064 - accuracy: 0.6576 - auc: 0.7041\n",
      "Epoch 00001: val_auc improved from -inf to 0.72700, saving model to /content/drive/MyDrive/Graduation_Project/model_weight_Tiamese_mobile_2.hdf5\n",
      "195/195 [==============================] - 138s 683ms/step - loss: 0.2064 - accuracy: 0.6576 - auc: 0.7041 - val_loss: 0.2476 - val_accuracy: 0.6948 - val_auc: 0.7270 - lr: 0.1000\n",
      "Epoch 2/100\n",
      "195/195 [==============================] - ETA: 0s - loss: 0.2030 - accuracy: 0.6682 - auc: 0.7078\n",
      "Epoch 00002: val_auc did not improve from 0.72700\n",
      "195/195 [==============================] - 123s 630ms/step - loss: 0.2030 - accuracy: 0.6682 - auc: 0.7078 - val_loss: 0.2074 - val_accuracy: 0.6703 - val_auc: 0.7021 - lr: 0.1000\n",
      "Epoch 3/100\n",
      "195/195 [==============================] - ETA: 0s - loss: 0.2011 - accuracy: 0.6704 - auc: 0.7082\n",
      "Epoch 00003: val_auc did not improve from 0.72700\n",
      "195/195 [==============================] - 123s 629ms/step - loss: 0.2011 - accuracy: 0.6704 - auc: 0.7082 - val_loss: 0.2154 - val_accuracy: 0.7076 - val_auc: 0.7108 - lr: 0.1000\n",
      "Epoch 4/100\n",
      "195/195 [==============================] - ETA: 0s - loss: 0.1981 - accuracy: 0.6754 - auc: 0.7137\n",
      "Epoch 00004: val_auc did not improve from 0.72700\n",
      "195/195 [==============================] - 123s 631ms/step - loss: 0.1981 - accuracy: 0.6754 - auc: 0.7137 - val_loss: 0.2061 - val_accuracy: 0.6888 - val_auc: 0.7168 - lr: 0.1000\n",
      "Epoch 5/100\n",
      "195/195 [==============================] - ETA: 0s - loss: 0.1981 - accuracy: 0.6751 - auc: 0.7150\n",
      "Epoch 00005: val_auc did not improve from 0.72700\n",
      "195/195 [==============================] - 123s 630ms/step - loss: 0.1981 - accuracy: 0.6751 - auc: 0.7150 - val_loss: 0.2021 - val_accuracy: 0.6990 - val_auc: 0.7095 - lr: 0.1000\n",
      "Epoch 6/100\n",
      "195/195 [==============================] - ETA: 0s - loss: 0.1982 - accuracy: 0.6726 - auc: 0.7148\n",
      "Epoch 00006: val_auc did not improve from 0.72700\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.010000000149011612.\n",
      "195/195 [==============================] - 123s 630ms/step - loss: 0.1982 - accuracy: 0.6726 - auc: 0.7148 - val_loss: 0.2128 - val_accuracy: 0.6827 - val_auc: 0.6939 - lr: 0.1000\n",
      "Epoch 7/100\n",
      "195/195 [==============================] - ETA: 0s - loss: 0.1978 - accuracy: 0.6775 - auc: 0.7093\n",
      "Epoch 00007: val_auc did not improve from 0.72700\n",
      "195/195 [==============================] - 123s 631ms/step - loss: 0.1978 - accuracy: 0.6775 - auc: 0.7093 - val_loss: 0.1966 - val_accuracy: 0.6674 - val_auc: 0.7159 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "195/195 [==============================] - ETA: 0s - loss: 0.1950 - accuracy: 0.6780 - auc: 0.7181\n",
      "Epoch 00008: val_auc did not improve from 0.72700\n",
      "195/195 [==============================] - 123s 630ms/step - loss: 0.1950 - accuracy: 0.6780 - auc: 0.7181 - val_loss: 0.1946 - val_accuracy: 0.6773 - val_auc: 0.7153 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "195/195 [==============================] - ETA: 0s - loss: 0.1959 - accuracy: 0.6790 - auc: 0.7146\n",
      "Epoch 00009: val_auc did not improve from 0.72700\n",
      "195/195 [==============================] - 123s 630ms/step - loss: 0.1959 - accuracy: 0.6790 - auc: 0.7146 - val_loss: 0.1934 - val_accuracy: 0.6789 - val_auc: 0.7255 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "195/195 [==============================] - ETA: 0s - loss: 0.1955 - accuracy: 0.6807 - auc: 0.7146\n",
      "Epoch 00010: val_auc did not improve from 0.72700\n",
      "195/195 [==============================] - 123s 631ms/step - loss: 0.1955 - accuracy: 0.6807 - auc: 0.7146 - val_loss: 0.1952 - val_accuracy: 0.6680 - val_auc: 0.7088 - lr: 0.0100\n",
      "Epoch 11/100\n",
      "195/195 [==============================] - ETA: 0s - loss: 0.1927 - accuracy: 0.6844 - auc: 0.7250\n",
      "Epoch 00011: val_auc did not improve from 0.72700\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "195/195 [==============================] - 123s 631ms/step - loss: 0.1927 - accuracy: 0.6844 - auc: 0.7250 - val_loss: 0.1911 - val_accuracy: 0.6808 - val_auc: 0.7260 - lr: 0.0100\n"
     ]
    }
   ],
   "source": [
    "history = siamese.fit(train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks_list\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nyZ1shdxYEg8"
   },
   "outputs": [],
   "source": [
    "def My_inputs(model,dir_image,threshold):\n",
    "  D_idx={ 'No_Finding':0, 'Enlarged_Cardiomediastinum':1, 'Cardiomegaly':2,\n",
    "       'Lung_Opacity':3, 'Lung_Lesion':4, 'Edema':5, 'Consolidation':6, 'Pneumonia':7,\n",
    "       'Atelectasis':8, 'Pneumothorax':9, 'Pleural_Effusion':10, 'Pleural_Other':11,\n",
    "       'Fracture':12, 'Support_Devices':13}\n",
    "  to_predict_image=cv2.imread(dir_image)\n",
    "  to_predict_image=cv2.resize(to_predict_image,(256,256))\n",
    "  to_predict_image=to_predict_image[np.newaxis,:,:,:]\n",
    "  prediction=np.zeros((14,1))\n",
    "\n",
    "  for d in os.listdir('/content/drive/MyDrive/Data/Val'):\n",
    "    preds=[]\n",
    "\n",
    "    for img in os.listdir('/content/drive/MyDrive/Data/Val/'+d):\n",
    "      my_img=cv2.imread('/content/drive/MyDrive/Data/Val/'+d+'/'+img)\n",
    "      my_img=cv2.resize(my_img,(256,256))\n",
    "      my_img=my_img[np.newaxis,:,:,:]\n",
    "      preds.append(model.predict([to_predict_image,my_img]))\n",
    "    prediction[D_idx[d]]=np.array(preds).mean()\n",
    "  return prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fS9EF4OBCRtl",
    "outputId": "e7468b6c-56ef-42a1-8c75-40b9d1061751"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10481144],\n",
       "       [0.00388822],\n",
       "       [0.00075505],\n",
       "       [0.00072143],\n",
       "       [0.02478264],\n",
       "       [0.02040445],\n",
       "       [0.00789811],\n",
       "       [0.01149207],\n",
       "       [0.00643973],\n",
       "       [0.00372105],\n",
       "       [0.00261437],\n",
       "       [0.03291192],\n",
       "       [0.00565439],\n",
       "       [0.00737246]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R=My_inputs(siamese,'/content/drive/MyDrive/Data/Val/No_Finding/No_Finding_101.jpg',0.2)\n",
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8hFtwvuCSdn",
    "outputId": "7f7c0dc3-0d8e-4669-bcde-7a232cbd0967"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02367543],\n",
       "       [0.0159646 ],\n",
       "       [0.09395885],\n",
       "       [0.00236193],\n",
       "       [0.03043787],\n",
       "       [0.01930635],\n",
       "       [0.01341991],\n",
       "       [0.0034225 ],\n",
       "       [0.0055633 ],\n",
       "       [0.0011952 ],\n",
       "       [0.0086239 ],\n",
       "       [0.03168651],\n",
       "       [0.003994  ],\n",
       "       [0.00717427]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2=My_inputs(siamese,'/content/drive/MyDrive/Data/Val/Cardiomegaly/Cardiomegaly_101.jpg',0.2)\n",
    "R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q3HsOjUHhars",
    "outputId": "19e358f2-95de-403f-91cd-7d24b39f42ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00981312],\n",
       "       [0.00934001],\n",
       "       [0.02268913],\n",
       "       [0.00242263],\n",
       "       [0.01622439],\n",
       "       [0.09319084],\n",
       "       [0.01102469],\n",
       "       [0.0056631 ],\n",
       "       [0.00421425],\n",
       "       [0.00228461],\n",
       "       [0.0125719 ],\n",
       "       [0.0513186 ],\n",
       "       [0.00573691],\n",
       "       [0.00672107]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R3=My_inputs(siamese,'/content/drive/MyDrive/Data/Val/Edema/Edema_107.jpg',0.2)\n",
    "R3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nP1cfrX5hk1W",
    "outputId": "0bf41f96-9b4e-45ae-f364-8b1b1cf7ac1f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.92931870e-03],\n",
       "       [4.17525735e-04],\n",
       "       [3.80573329e-05],\n",
       "       [6.67607132e-03],\n",
       "       [1.11673505e-03],\n",
       "       [2.45329551e-03],\n",
       "       [1.08795666e-05],\n",
       "       [8.44141468e-02],\n",
       "       [1.24711054e-03],\n",
       "       [1.95492612e-04],\n",
       "       [2.28310615e-04],\n",
       "       [5.04220650e-03],\n",
       "       [3.78303463e-03],\n",
       "       [4.93039470e-03]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R4=My_inputs(siamese,'/content/drive/MyDrive/Data/Val/Pneumonia/Pneumonia_101.jpg',0.2)\n",
    "R4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TBHfekwahxH-",
    "outputId": "5c6a00b2-fd31-44b3-b95c-68c267f9eb63"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.85959429e-04],\n",
       "       [2.86779774e-04],\n",
       "       [1.73172171e-04],\n",
       "       [1.45328013e-04],\n",
       "       [3.97006836e-04],\n",
       "       [4.80900344e-04],\n",
       "       [7.92012579e-05],\n",
       "       [7.03038677e-05],\n",
       "       [5.10444923e-04],\n",
       "       [2.36833894e-05],\n",
       "       [8.30149353e-02],\n",
       "       [9.81115620e-04],\n",
       "       [1.54916255e-03],\n",
       "       [1.13958587e-04]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R5=My_inputs(siamese,'/content/drive/MyDrive/Data/Val/Pleural_Effusion/Pleural_Effusion_102.jpg',0.2)\n",
    "R5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AlIVYecqiWbu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Tiamese_v8.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
